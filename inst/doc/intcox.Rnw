
%\VignetteIndexEntry{intcox: Compendium to apply the iterative convex minorant algorithm to interval censored event data}
%\VignetteDepends{survival}
%\VignetteKeywords{interval censoring, proportional hazards, event data}
%\VignettePackage{intcox}


% Notes
% - the package 'intcox' must be installed
% - do not close the x11() window which is opened and used by Sweave

\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{graphics}
\usepackage{epsfig}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\begin{document}
\title{intcox: Compendium to apply the iterative convex minorant
algorithm to interval censored event data}
\author{Volkmar Henschel, Christiane Hei{\ss}, Ulrich Mansmann\\
University of Heidelberg\\
Department of Medical Biometry and Informatics\\
INF 305, 69120 Heidelberg, Germany}
\maketitle
\begin{abstract}
Software that fits a multivariate proportional hazards model to
interval censored event data is urgently needed in medical
research. Pan describes the use of the iterative convex minorant
algorithm (ICM) to achieve this purpose. The implementation of the
ICM is presented as well as a bootstrap procedure to derive
information for statistical inference on the regression
coefficients. An example is studied and the \texttt{intcox}
results are compared to results of alternative procedures
available in commercial software products.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The occurrence of an event represents important information on the
prognosis or treatment efficacy of a disease. It often plays the
role of the primary endpoint in clinical studies. But, like the
recurrence of a tumor, the time of its occurrence cannot be
exactly observed. Events which are known to occur only within
intervals represent interval censored event data.\\
Especially of interest is the influence of a covariate vector x on
the probability of the occurrence of events which is formalized by
the survival function $S(t|x)=1-F(t|x)$ with $F$ the cumulative 
distribution function.\\
In case of right censored event data, the quantification is
achieved by applying the extended proportional hazard model of
Cox, cf. Therneau and Grambsch \cite{therneau00} which makes the
following assumption on the survival function
\begin{align*}
    S(t|\mathbf{x})&=\exp\{-\Lambda(t|\mathbf{x})\}\\
    &=\exp\left\{-\int_0^t\lambda_0(s)
    \exp\{\boldsymbol\beta'\mathbf{x}\}\mathrm{d}s\right\}.
\end{align*}
The expression $\Lambda(t|\mathbf{x})$ is called the cumulative
hazard which is the integral of the hazard function
$\lambda(s|\mathbf{x})$ up to time $t$
\[\lambda (s,\mathbf{x})=\lambda _0(s)\exp\{\boldsymbol\beta'\mathbf{x}\}.\]
The model is called proportional hazards model because the
regression coefficients act via a factor on the hazard function.
The exponential value of a component of the coefficient vector is
called relative hazard of the corresponding factor.\\
In Pan \cite{pan99}, a proportional hazard model is fit to
interval censored data by means of the iterative convex minorant
algorithm (ICM). This model allows to infer relative hazards from
interval censored event data.\\
The omnipresence of multivariate interval censored data in medical
research creates a strong need for appropriate software. In spite
of the high practical relevance, the algorithm of Pan is not
implemented in a statistical software environment and available
for public use. In this note, we describe an implementation of the
ICM-based algorithm for interval censored event data in the R
software.\\
After a short description of the mathematical background, a
clinical example of interval censored event data is presented and
analyzed. The results are compared to alternative procedures
available in commercial software products.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Mathematical Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The proportional hazards assumption combines the covariate vector
$\mathbf{x}$ and the vector of regression coefficients
$\boldsymbol\beta$ via a linear predictor with the baseline hazard
$\lambda _0(t)$:
\[\lambda (t,\mathbf{x})=\lambda _0(t)\exp\{\boldsymbol\beta'\mathbf{x}\}.\]
A straightforward calculation gives the likelihood contribution of
an observation which takes place in the interval $[s,t]$ and
allows to find the log-likelihood of the data
\begin{align*}
L(F_0,\boldsymbol\beta) =&\sum_{i=1}^n\ln\Bigl\{
(1-F_0(S_i-))^{\exp(\boldsymbol\beta'\mathbf{x}_i)}
-(1-F_0(T_i))^{\exp(\boldsymbol\beta'\mathbf{x}_i)} \Bigr\}.
\end{align*}
This calculations uses the following relationship between survival
and distribution function in the proportional hazards model
\begin{align*}
    (1-F(t|\mathbf{x}))&=S(t|\mathbf{x})\\
    &=S_0(t|\mathbf{x})^{\exp\{\boldsymbol\beta'\mathbf{x}\}}\\
    &=(1-F_0(t|\mathbf{x}))^{\exp\{\boldsymbol\beta'\mathbf{x}\}}.
\end{align*}
The objective of the ICM-algorithm is to maximize the
log-likelihood by a modified Newton-Raphson algorithm. The
gradients needed for the maximization are $\nabla_1
L(F_0,\boldsymbol\beta)=\frac{\partial
L(F_0,\boldsymbol\beta)}{\partial F_0}$ and $\nabla_2
L(F_0,\boldsymbol\beta)=\frac{\partial
L(F_0,\boldsymbol\beta)}{\partial\boldsymbol\beta}$. The baseline
distribution function $F_0$ is considered to be piecewise constant
and thus can be represented by a finite dimensional vector which
is parameterized by the finite steps of the cumulative baseline
hazard function. The derivative with respect to $F_0$ is the
gradient of the log-likelihood with respect to the vector of the
baseline cumulative distribution function values. The derivative
with respect to $\boldsymbol\beta$ is the usual derivative of the
log-likelihood with respect to the components of beta. The full
Hessian in the in the original Newton-Raphson algorithm is
replaced by the diagonal matrices of the negative second
derivatives $G_1(F_0,\boldsymbol\beta)$ and
$G_2(F_0,\boldsymbol\beta)$. \\
The update from $F^{(m)}$ to $F^{(m+1)}$ is done iteratively with
control of the stepsize. Starting point is always a stepsize of
$\alpha=1$. The new candidates for $F_0$ and $\boldsymbol\beta$
result from
\begin{align*}
F_0^{(m+1)}=&\mathrm{Proj} \Bigl[F_0^{(m)}
+\alpha G_1(m)^{-1}\nabla_1 L(m), G_1(m),\mathcal{R}\Bigr]\\
\boldsymbol\beta^{(m+1)}=&\boldsymbol\beta^{(m)}+\alpha
G_2(m)^{-1}\nabla_2 L(m).
\end{align*}
A projection into the restricted range $\mathcal{R}$ weighted by
$G$ is used to assure that $F_0^{(m+1)}$ is again a distribution
function:
\begin{align*}
\mathrm{Proj}[y,G,\mathcal{R}]=&\arg\min_x
\Bigl\{\sum_{i=1}^k(y_i-x_i)^2G_{ii}: 0\leq x_1\leq\dots\leq
x_k\leq1\Bigr\}.
\end{align*}
In case of $L(F^{(m+1)}) <L(F^{(m)})$, $\alpha$ is halved and the
step is reiterated. A numerical procedure for the restricted
projection is the pool adjacent violators algorithm (PAVA), which
is described in Robertson et al. \cite{robertson88}.\\
Starting values are calculated by treating the data as right
censored and using the classical proportional hazards model. An
event within a bounded interval $[s,t]$ will be interpreted as
event observed at time $t$. An event within an interval unbounded
to the right $[s,\infty]$ will be interpreted as a right censored
event at time $s$. The Breslow-estimator is used to get a starting
value for the baseline hazard $\Lambda_0(t)$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Meisel et al. \cite{meisel00} present data on the shrinkage of
aneurisms associated with cerebral arteriovenous malformations
(cAVM) after embolization treatment. The time to a shrinkage of
the aneurism to below 50\verb-%- of the baseline volume was of
interest. Several patients had multiple aneurisms. Each patient
was inspected at a random inspection time $obs.t$. The censoring
variable $z$ was set to one, if at the inspection time sufficient
shrinkage was observed, else the censoring indicator was set to
zero.\\
Two covariates were considered: the degree of cAMV occlusion by
embolization (dichotomized at 50\verb-%-, variable $mo$)
and the location of the aneurism, whether at the midline arteries
or at other afferent cerebral arteries, variable $lok$.\\
The single aneurisms are not independent because aneurisms within
a patient may shrink in the same way (because the share the same
"environment"). Multiple aneurisms were observed per patient. This
clustering of aneurisms is indicated by the grouping variable
$gr$.\\
The data is loaded and inspected for the first five patients.
<<aneurism load>>= 
library(survival) 
library(intcox) 
data(AA.data)
AA.data[1:11,] 
@ 
The data is analyzed by applying the
\texttt{intcox} algorithm. The algorithm requires the interval
censored observation given in interval format with left and right
boundaries. The \texttt{Surv} function does not allow the use of
current status censored data as given in the aneurism example. In
case of right censored data the right boundaries are set arbitrary
to \texttt{NA}. 
<<aneurism preparation>>=
AA.data$t.left<-ifelse(AA.data$z==1,0,AA.data$obs.t)
AA.data$t.right<-ifelse(AA.data$z==1,AA.data$obs.t,NA)
@
The fit with \texttt{intcox} gives an object of class "coxph"
without the standard errors of the regression coefficients. The
summary function for class "coxph" allows to summarize the output
of the estimation procedure.
<<aneurism fit>>=
AA.fit<-intcox(Surv(t.left,t.right,type="interval2")~mo+lok,data=AA.data)
summary(AA.fit)
@
Additionally to the components of a class "coxph" object there are
given:
\begin{description}
\item
lambda0: the estimated cumulative baseline hazard;
\item
time.point: the corresponding time points at which the cumulative
baseline hazard was estimated;
\item
likeli.vec: a vector of the estimated loglik of each ICM step;
\item
termination: an indicator for the reason of termination (see HTML
Help for details), 1 indicates that the algorithm converged.
\end{description}
The estimated coefficients and cumulative baseline hazard can be
used to estimated and plot group specific survival curves.
<<aneurismplot, fig=TRUE>>=
surv.base<-exp(-AA.fit$lambda0)
plot(AA.fit$time.point,surv.base,type="s",xlab="time [years]",ylab="Survival function",lty=1)
lines(AA.fit$time.point,surv.base^exp(AA.fit$coefficients["mo"]),type="s",lty=2)
lines(AA.fit$time.point,surv.base^exp(AA.fit$coefficients["lok"]),type="s",lty=3)
lines(AA.fit$time.point,surv.base^exp(sum(AA.fit$coefficients[c("mo","lok")])),type="s",lty=5)
leg.names<-c("mo=0, lok=0", "mo=0, lok=1","mo=1, lok=0","mo=1, lok=1")
legend(4,1,leg.names,lty=c(1,2,3,5),bty="n")
@

It is of interest to calculate basic bootstrap confidence
intervals \cite{davison97} of the regression coefficients. A wild
bootstrap procedure is used, c.f. Burr \cite{burr94}. Because
several patients present with multiple aneurisms, the bootstrap
respected this clustering by sampling patients and not individual
aneurisms. A patient enters with all of her/his aneurisms in the
analysis. This procedure is in accordance with the analysis of
marginal models as presented in chapter 8 of Therneau and Grambsch
\cite{therneau00}. The bias between the ICM estimates and the
median/mean of the bootstrap samples will also be assessed.
\begin{remark}
The number of replicates should be set to at least 999. The 
low number of nine is only chosen for a fast check by CRAN.
\end{remark}
<<aneurism bootstrap>>=
set.seed(123)
pat<-unique(AA.data$gr)
intcox.boot.AA<-function(i,form) {
   boot.sample<-sample(pat,length(pat),replace=T)
   data.ind<-unlist(lapply(boot.sample,function(x,yy) which(yy==x),yy=AA.data$gr))
   data.sample<-AA.data[data.ind,]
   boot.fit<-intcox(form,data=data.sample, no.warnings = TRUE)
   return(list(coef=coef(boot.fit),term=boot.fit$termination))
   }
n.rep<-9                                #to be set on something like 999
AA.boot<-lapply(1:n.rep,intcox.boot.AA,form=Surv(t.left,t.right,type="interval2")~mo+lok)
AA.boot<-matrix(unlist(AA.boot),byrow=T,nrow=n.rep)
colnames(AA.boot)<-c(names(coef(AA.fit)),"termination")
inf.level<-0.05 
mo.ord<-order(AA.boot[,"mo"])
lok.ord<-order(AA.boot[,"lok"])
pos.lower<-ceiling((n.rep+1)*(inf.level/2))
pos.upper<-ceiling((n.rep+1)*(1-inf.level/2))
ci.mo<-AA.boot[mo.ord,"mo"][c(pos.lower,pos.upper)]
ci.lok<-AA.boot[mo.ord,"lok"][c(pos.lower,pos.upper)] 
ci.mo 
ci.lok
summary(AA.boot[,"mo"]) 
summary(AA.boot[,"lok"])
bias.mo<-c(mean.bias=coef(AA.fit)["mo"]-mean(AA.boot[,"mo"]),median.bias=coef(AA.fit)["mo"]-median(AA.boot[,"mo"]))
bias.lok<-c(mean.bias=coef(AA.fit)["lok"]-mean(AA.boot[,"lok"]),median.bias=coef(AA.fit)["lok"]-median(AA.boot[,"lok"]))
bias.mo 
bias.lok 
table(AA.boot[,"termination"])
@
The analysis shows a light bias between the the ICM estimates on
the original data and the median/mean of the bootstrap samples
which will not invalidate the statistical inference on the
regression coefficient based on the basic bootstrap confidence
interval which show the significant (level $\alpha=0.05$)
influence of both covariates on the shrinkage.\\
There are some caveats:
\begin{description}
\item
Single group setting: A single group model can not be calculated
by the ICM algorithm. The calculation needs explicitly one or more
covariates.
\item
Stop after first step: There are situations where the ICM algorithm
does not iterate. Then the starting values calculated from Cox
model by interpreting the data as right censored give a likelihood
value which can not be improved by the ICM algorithm. The
occurrence of this situation will be announced by a warning.
\item
Model selection: Look at the following model selection problem:
Does the model \texttt{Surv(t.left,t.right,type="interval2")$\sim$
mo*lok} improve the fit to the data. One way to answer this
question is to check the bootstrap confidence interval for the
interaction regression coefficient.
The calculation below renders a 95\verb-%- bootstrap confidence
interval of $[-1.22; 1.77]$ and a small bias which does not
influences the statistical decision. One can conclude that there
is no significant interaction between the covariates with respect
to shrinkage.
\begin{remark}
The number of replicates should be set to at least 999. The 
low number of nine is only chosen for a fast check by CRAN.
\end{remark}
<<aneurism interaction>>=
AA.int.fit<-intcox(Surv(t.left,t.right,type="interval2")~mo*lok,data=AA.data)
summary(AA.int.fit)
set.seed(234)
n.rep<-9                                #to be set on something like 999
AA.int.boot<-lapply(1:n.rep,intcox.boot.AA,form=Surv(t.left,t.right,type="interval2")~mo*lok)
AA.int.boot<-matrix(unlist(AA.int.boot),byrow=T,nrow=n.rep)
colnames(AA.int.boot)<-c(names(coef(AA.int.fit)),"termination")
inf.level<-0.05
int.ord<-order(AA.int.boot[,"mo:lok"])
pos.lower<-ceiling((n.rep+1)*(inf.level/2))
pos.upper<-ceiling((n.rep+1)*(1-inf.level/2))
ci.int<-AA.int.boot[int.ord,"mo:lok"][c(pos.lower,pos.upper)]
ci.int
summary(AA.int.boot[,"mo:lok"])
bias.int<-c(mean.bias=coef(AA.int.fit)["mo:lok"]-mean(AA.int.boot[,"mo:lok"]),median.bias=coef(AA.int.fit)["mo:lok"]-median(AA.int.boot[,"mo:lok"]))
bias.int
table(AA.int.boot[,"termination"])
@
\item
Likelihood ratio test: There are caveats when using the calculated
likelihood to perform a likelihood ratio test for model selection.
First, we are not aware of an asymptotic theory for this test,
second, the likelihood of the larger model may be smaller as the
likelihood of the sparser model as can be seen in our example.
<<aneurism LRT>>=
AA.int.fit$loglik
AA.fit$loglik
@
\end{description}
A non-parametric approach to the data uses Turnbull's
\cite{turnbull74} generalization of the Kaplan-Meier estimator
which is implemented in S-Plus (Insightful Corporation). The
following S-Plus code will not work in R.
\begin{verbatim}
> surv.formula<-censor(left,right,cens*3,type="interval")~mo+lok
> plot(kaplanMeier(surv.formula,data=aneur),lty=c(1,2,3,5),
+ xlab="time (years)",ylab="Survival function")
\end{verbatim}
Figure \ref{aneurkm} shows the results when applied to the four
subgroups of patients.
\begin{figure}
\begin{center}
\includegraphics{aneur_km}\caption{Turnbull's
generalization of the Kaplan-Meier estimator}\label{aneurkm}
\end{center}
\end{figure}
The fitting of an accelerated failure time model for interval
censored data could be carried out by means of the SAS-procedure
LIFEREG (SAS-Institute Inc. Cary, North Carolina, USA). Because of
technical reasons, the value 0 for the lower end of an interval
had to be replaced by the equivalent of 1 day: 1/365. The Weibull
distribution was chosen for the error term. Then the estimates of
the coefficients can be interpreted as log-transformed relative
hazards:  mo -1.64 [-2.88, -0.57], lok -1.22 [-2.21, -0.36]. The
baseline hazard is determined by intercept 0.47 [-0.36, 1.37] and
scale 1.67 [1.25, 2.01].\\
To reproduce the calculation, the dataset AA.data (columns
separated by a semicolon) is written as comma seperated value file
"AA.csv" to the working directory (shown by the getwd call) and is
available for the given SAS procedure.
<<aneurism export>>=
getwd()
write.table(AA.data, file = "AA.csv", sep = ",", na = "." , col.names = NA)
@
\begin{verbatim}
PROC IMPORT OUT= WORK.aneur
            DATAFILE= "Pfad  \AA.csv"
            DBMS=CSV REPLACE;
     GETNAMES=YES;
     DATAROW=2;
RUN;
data aneur;
    set aneur;
    if t_left=0 then t_left=1/365.25;
    gruppe=2*mo+lok;
run;
proc lifereg data=aneur;
    model (t_left,t_right)=mo lok / d=weibull;
    output out=outcdf cdf=cdf;
run;
data outcdf;
    set outcdf;
    svf=1-cdf;
run;
proc sort data=outcdf;
    by gruppe svf;
run;
symbol1 color=black i=spline line=1;
symbol2 color=black i=spline line=33;
symbol3 color=black i=spline line=41;
symbol4 color=black i=spline line=43;
AXIS1 label=none MINOR=(number=1)
LABEL=(justify=center angle=90 Rotate=0 'Survival function');
AXIS2 Label=none MINOR=(number=1) order=(0 to 9 by 1)
LABEL=(JUSTIFY=CENTER 'time (years)');
LEGEND across=1 label=none position=(top right inside)
value=('mo=0, lok=0' 'mo=0, lok=1' 'mo=1, lok=0' 'mo=1, lok=1');
proc gplot data=outcdf;
    plot svf*t_left=gruppe / vaxis=axis1 haxis=axis2 legend=legend;
run; quit;
goptions reset=all;
\end{verbatim}
Figure \ref{aneurpara} represents the result of the Weibull approach.
\begin{figure}[t]
\begin{center}
\includegraphics{aneur_para}\caption{Accelerated
failure time model}\label{aneurpara}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The ICM-based algorithm of Pan follows the maximum likelihood
rationale of estimation. The computational effort is low, because
only the diagonal elements of the Hessian matrix are used, as well
as the PAVA for the restricted regression. The ICM-algorithm is
implemented in a R-package which includes a dynamically loaded
C-routine for the PAVA. It has to be used with care.\\
Simulation studies showed a light positive bias in the estimated
regression coefficients. From a practical point of view, this bias
seems to be acceptable but has to be taken into consideration when
the results of a study will be interpreted. The ICM gives a rather
rough estimate of the baseline hazard, because the PAVA smoothes
this function to large intervals of constant hazard. As an example
the obtained bias of the coefficients of a bootstrap of 999 samples 
from the \texttt{intcox.example}, see HTML Help, is shown in figure 
\ref{simu}.\\
\begin{figure}
\begin{center}
\includegraphics{comb1einzel}\caption{Bias in the coefficients of 
intcox.example}\label{simu}
\end{center}
\end{figure}
The algorithm is able to handle the combination of real right
censored data and real interval censored data. It is not wise to
transfer right censored data artificially to interval censored
data by substituting a large number as right endpoint of a right
censored event. This introduces bias into the relative hazards
estimates.\\
We observed problems in the algorithm with respect to maximizing
the likelihood in case of a high percentage of right censored data
($>30$\verb-%-).
In this case, the algorithm does not move away from the starting
values calculated from the Cox model as described in the first
paragraph of section 2.\\
This work was supported by DFG grant MA 1723/2-1.
\begin{thebibliography}{6}
\bibitem{burr94}
Deborah Burr.
\newblock A comparison of certain bootstrap confidence intervals in the cox
  modell.
\newblock {\em Journal of the American Statistical Association}, 89:1290--1302,
  1994.
\bibitem{davison97}
A.~C. Davison and D.~V. Hinkley.
\newblock {\em Bootstrap methods and their application}.
\newblock Cambridge University Press, Cambridge, 1997.
\bibitem{meisel00}
H.~J. Meisel, U.~Mansmann, H.~Alvarez, G.~Rodesch, M.~Brock, and
P.~Lasjaunias.
\newblock Cerebral arteriovenous malformations and associated aneurysms:
  Analysis of 305 cases from a series of 662 patients.
\newblock {\em Neurosurgery}, 46:793--802, 2000.
\bibitem{pan99}
Wei Pan.
\newblock Extending the iterative convex minorant algorithm to the cox model
  for interval-censored data.
\newblock {\em Journal of Computational and Graphical Statistics}, 78:109--120,
  1999.
\bibitem{robertson88}
Tim Robertson, F.T. Wright, and Richard~L. Dykstra.
\newblock {\em Order Restricted Statistical Inference}.
\newblock Wiley, New York, 1988.
\bibitem{therneau00}
Terry~M. Therneau and Patricia~M. Grambsch.
\newblock {\em Modeling Survival Data: extending the Cox model}.
\newblock Springer, New York, 2000.
\bibitem{turnbull74}
B.~Turnbull.
\newblock Nonparametric estimation of a survivorship function with doubly
  censored data.
\newblock {\em Journal of the American Statistical Association}, 69:169--173,
  1974.
\end{thebibliography}
\end{document}
